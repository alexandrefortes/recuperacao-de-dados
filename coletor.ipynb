{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Especialização em Inteligência Artificial – IFMG\n",
    "## Recuperação da Informação\n",
    "**Aluno**: Alexandre Fortes Santana  \n",
    "\n",
    "**Professor**: Moisés Ramos\n",
    "\n",
    "[Notebook: https://github.com/alexandrefortes/recuperacao-de-dados/blob/main/coletor.ipynb](https://github.com/alexandrefortes/recuperacao-de-dados/blob/main/coletor.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planejamento e Definição de Escopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escopo\n",
    "# - Indexar conteúdo do portal da Efí Bank. Pretendo usar o conteúdo para treinar um bot GPT para atendimento nível 1 a clientes.\n",
    "\n",
    "# Seleção do Coletor\n",
    "# - Para o projeto de indexar o conteúdo do portal da Efí Bank com o objetivo de treinar um bot GPT, Scrapy parece ser a melhor escolha inicial devido à sua facilidade de uso, documentação extensa, e flexibilidade para customizações.\n",
    "\n",
    "# Inicialização do Coletor\n",
    "# - Configurar Scrapy, adaptando-o ao escopo.\n",
    "\n",
    "# Escalonador de URL\n",
    "# - Implementar mecanismo para gerenciar a fila de URLs a serem visitadas, incluindo políticas de seleção baseadas em critérios de qualidade e relevância.\n",
    "# O Scrapy possui um componente de escalonamento de URL interno chamado de Scheduler, que gerencia a ordem e a prioridade dos requests.\n",
    "# Filtragem de Duplicatas: Por padrão, o Scrapy filtra requests duplicadas para a mesma URL, evitando o processamento múltiplo da mesma página.\n",
    "\n",
    "# Buscador (Fetcher)\n",
    "# - Desenvolver a funcionalidade para realizar o download das páginas utilizando as URLs, lidando com os protocolos HTTP/HTTPS e o tratamento de erros de conexão.\n",
    "# O componente responsável por fazer o download das páginas no Scrapy é conhecido como Downloader.\n",
    "\n",
    "# Extrator de Links (Link Extractor)\n",
    "# - Criar um componente para extrair e normalizar links das páginas baixadas, enviando-os de volta ao escalonador para futuras visitas.\n",
    "# O Scrapy possui um componente específico chamado LinkExtractor que é usado justamente para extrair links de uma página baixada. Ele permite a extração e normalização dos links automaticamente, facilitando o processo de seguir links para outras páginas dentro do site alvo. \n",
    "# Extrair Links: definir regras dentro do LinkExtractor para especificar quais links devem ser seguidos, baseado em expressões regulares, tags e atributos. \n",
    "# Normalização de URLs: O Scrapy automaticamente normaliza os links extraídos, convertendo URLs relativas em URLs absolutas e lidando com peculiaridades como redirecionamentos e parâmetros de URL.\n",
    "\n",
    "\n",
    "# Analisador de Conteúdo (Content Parser)\n",
    "# - Implementar a análise do conteúdo das páginas, extraindo informações relevantes e lidando com diferentes formatos (HTML, PDF, RSS, etc.).\n",
    "# Scrapy é bom na análise de conteúdo de páginas web, principalmente HTML e XML.\n",
    "# Parsing de HTML/XML: Através do uso de seletores CSS ou XPath, permite extrair facilmente dados estruturados. \n",
    "# Pipelines de Item: Após extrair os dados, processá-los e normalizá-los usando os pipelines de item, para limpeza de dados, validação, e armazenamento.\n",
    "\n",
    "# Armazenamento (Storage)\n",
    "# - Criar um pipeline que escreve/armazena cada item extraído em dataframes.\n",
    "# - Definir a arquitetura do dataframe.\n",
    "\n",
    "# Políticas de Revisita para Arquivamento\n",
    "# Mensal.\n",
    "\n",
    "# Obediência aos Protocolos de Exclusão de Robôs\n",
    "# - Garantir que o coletor respeite o arquivo robots.txt e meta-tags de exclusão, implementando a leitura e obediência a essas diretivas.\n",
    "\n",
    "# Boas Maneiras na Coleta\n",
    "# - Desenvolver mecanismo para evitar a sobrecarga ao portal, incluindo controle da frequência de acessos e identificação transparente do agente.\n",
    "\n",
    "# Performance e Escalabilidade\n",
    "# - Adotar técnica para a coleta distribuída e paralela, visando escalabilidade.\n",
    "\n",
    "# Tratamento de Conteúdo Dinâmico e Erros\n",
    "# - Implementar estratégia para tratar conteúdos dinâmicos e formulários web, além do tratamento de páginas malformadas e erros de conteúdo.\n",
    "\n",
    "# Testes e Validação\n",
    "# - Realizar testes para validar a eficácia das políticas de seleção, revisita e aderência aos protocolos de exclusão de robôs.\n",
    "\n",
    "# Documentação e Manutenção\n",
    "# - Documentar a arquitetura da aplicação, as políticas adotadas, e o guia de uso, estabelecendo um plano de manutenção e atualização contínua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pra depois:\n",
    "- Armazenamento em elasticsearch? https://www.elastic.co/pt/downloads/elasticsearch\n",
    "- Criar mecanismo para decidir a frequência de revisita das páginas, baseado na estimativa da frequência de atualização."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
